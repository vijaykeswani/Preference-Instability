## Problems of Aligning AI With Temporally Unstable Human Feedback

This repository contains analysis of the large-scale data collected to assess moral preference instability. Like Studies 1 & 2, participants were provided with several pairwise comparisons of kidney patients and asked to decide which of the presented patients should receive an available kidney. The complete details of the data collection process are provided in the reference below,

The notebook `main_analysis.ipynb` contains details of the analysis of the response and model instability of the recruited participants. Additionally, the notebook also contains the analysis of AI alignment methods in this setting of preference instability. All results reported in the paper below are generated using this notebook.


### Reference

[Moral Change or Noise? On Problems of Aligning AI With Temporally UnstableHuman Feedback] <br>
Vijay Keswani, Cyrus Cousins, Breanna Nguyen, Vincent Conitzer, Hoda Heidari, Jana Schaich Borg, Walter Sinnott-Armstrong<br>
*AAAI Alignment Track*, 2026

